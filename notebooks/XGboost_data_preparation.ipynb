{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:18:19.762434Z",
     "start_time": "2025-01-06T20:18:19.758843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd"
   ],
   "id": "70474cd3ef782cf2",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loading\n",
    "The following datasets are loaded into the notebook:\n",
    "1. **Product Catalog**: Contains detailed information about each product.\n",
    "2. **Transactions**: Includes all purchase transactions with customer and product information.\n",
    "3. **Product Category Map**: Maps products to their respective categories.\n",
    "4. **Test Data**: Dataset for final predictions.\n",
    "\n",
    "The initial few rows of each dataset are displayed to confirm successful loading and understand the structure of the data.\n"
   ],
   "id": "f901d66b26f9b6fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:18:20.019130Z",
     "start_time": "2025-01-06T20:18:19.790398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading datasets\n",
    "product_catalog = pd.read_csv('../data/product_catalog.csv')\n",
    "transactions = pd.read_csv('../data//transactions.csv')\n",
    "product_category_map = pd.read_csv('../data//product_category_map.csv')\n",
    "test = pd.read_csv('../data//test.csv')"
   ],
   "id": "b2d978cb2739671b",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "6c04c790df588618"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:18:30.490597Z",
     "start_time": "2025-01-06T20:18:20.020057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. **Convert `purchase_date` to datetime format**\n",
    "# Ensures date-related calculations can be performed.\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "\n",
    "# 2. **Group by `customer_id` and `product_id`**\n",
    "# Aggregates key metrics such as first purchase, last purchase, and total purchase count.\n",
    "grouped = transactions.groupby(['customer_id', 'product_id']).agg(\n",
    "    first_purchase=('purchase_date', 'min'),\n",
    "    last_purchase=('purchase_date', 'max'),\n",
    "    total_purchases=('purchase_date', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# 3. **Calculate second last purchase date**\n",
    "# Helper function to find the second last purchase date for each customer-product pair.\n",
    "def get_second_last_purchase(dates):\n",
    "    if len(dates) > 1:\n",
    "        return sorted(dates)[-2]  # Return the second last date if more than one purchase exists.\n",
    "    return pd.NaT  # Return NaT (Not a Time) if there is only one purchase.\n",
    "\n",
    "second_last_purchase = (\n",
    "    transactions.groupby(['customer_id', 'product_id'])['purchase_date']\n",
    "    .apply(get_second_last_purchase)\n",
    "    .reset_index(name='second_last_purchase')\n",
    ")\n",
    "\n",
    "# Merge the second last purchase date into the grouped DataFrame.\n",
    "grouped = grouped.merge(second_last_purchase, on=['customer_id', 'product_id'], how='left')\n",
    "\n",
    "# 4. **Calculate average days between purchases**\n",
    "# Computes the average interval between purchases for each customer-product pair.\n",
    "grouped['average_days_between_purchases'] = (\n",
    "    (grouped['last_purchase'] - grouped['first_purchase']).dt.days /\n",
    "    (grouped['total_purchases'] - 1)\n",
    ").fillna(0)  # Fill NaN with 0 for cases with only one purchase.\n",
    "\n",
    "# 5. **Filter pairs with at least two purchases**\n",
    "# Retains only customer-product pairs with 2 or more total purchases.\n",
    "grouped = grouped[grouped['total_purchases'] >= 2]\n",
    "\n",
    "# 6. **Calculate week codes**\n",
    "# Calculates the number of weeks since a reference date for the last purchase.\n",
    "reference_date = pd.Timestamp('2020-12-25')\n",
    "grouped['week_code'] = ((grouped['last_purchase'] - reference_date).dt.days // 7)\n",
    "grouped['week_code'] = grouped['week_code'].replace(5, 4)  # Adjust outliers in week codes.\n",
    "\n",
    "# 7. **Calculate product popularity**\n",
    "# Maps product popularity based on the total number of purchases.\n",
    "grouped['product_id_count'] = grouped['product_id'].map(grouped['product_id'].value_counts())\n",
    "grouped['popularity'] = pd.qcut(\n",
    "    grouped['product_id_count'],\n",
    "    q=10,  # Divide into 10 quantile bins\n",
    "    labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    ").astype(int)\n",
    "\n",
    "# 8. **Apply additional filters**\n",
    "# Filters based on logical conditions for purchase count and week codes.\n",
    "valid_filter = (\n",
    "    (grouped['total_purchases'] > 2) & (grouped['week_code'] > 0) |\n",
    "    (grouped['total_purchases'] > 1) & (grouped['week_code'] <= 0)\n",
    ")\n",
    "grouped = grouped[valid_filter]\n",
    "\n",
    "# 9. **Calculate predictions and adjust last purchase dates**\n",
    "# Predictions based on week codes, and adjusts the last purchase date using the second last purchase if applicable.\n",
    "grouped['prediction'] = grouped['week_code'].apply(lambda x: x if x > 0 else 0)\n",
    "grouped['last_last_purchase'] = grouped.apply(\n",
    "    lambda row: row['second_last_purchase'] if row['prediction'] != 0 else row['last_purchase'], axis=1\n",
    ")\n",
    "\n",
    "# 10. **Calculate second week code**\n",
    "# Computes week code for the second last purchase date.\n",
    "grouped = grouped[grouped['second_last_purchase'] < '2021-01-01']\n",
    "grouped['week_code_2'] = ((grouped['second_last_purchase'] - reference_date).dt.days // 7)\n",
    "\n",
    "# 11. **Final cleanup**\n",
    "# Drops unnecessary columns and renames 'week_code_2' for clarity.\n",
    "final_df = grouped.drop(columns=[\n",
    "    'product_id', 'customer_id', 'first_purchase', 'last_purchase',\n",
    "    'total_purchases', 'second_last_purchase', 'week_code', 'product_id_count', 'last_last_purchase'\n",
    "])\n",
    "final_df['popularity'] = final_df['popularity'].astype(int)\n",
    "final_df.rename(columns={'week_code_2': 'last_purchase_week_code'}, inplace=True)\n",
    "\n",
    "# Output the final DataFrame\n",
    "train_set = final_df\n",
    "\n",
    "# Save the prepared test set to a CSV file\n",
    "train_set.to_csv('../processed_data/preprocessed_train_set.csv', index=False)\n",
    "\n",
    "print(\"Train set has been saved successfully as 'preprocessed_test_set.csv'\")\n"
   ],
   "id": "453121978efa53ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has been saved successfully as 'preprocessed_test_set.csv'\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:18:30.494121Z",
     "start_time": "2025-01-06T20:18:30.491561Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3eb0acbc1f482910",
   "outputs": [],
   "execution_count": 109
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
