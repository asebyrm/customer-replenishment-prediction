{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:03:59.253515Z",
     "start_time": "2025-01-06T19:03:59.248230Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_steps = 8\n",
    "num_steps, num_features = n_steps, 1\n",
    "size = 0.1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:03:59.743400Z",
     "start_time": "2025-01-06T19:03:59.741715Z"
    }
   },
   "id": "e8af8fc80fe75b75",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len merge: 896426\n",
      "len merge: 89642\n"
     ]
    }
   ],
   "source": [
    "# Load the required datasets\n",
    "merged_data = pd.read_csv('../processed_data/merged_data.csv')  # Main dataset containing merged data\n",
    "catalog = pd.read_csv('../data/product_catalog.csv')  # Product catalog dataset\n",
    "customer_features = pd.read_csv('../processed_data/customer_features.csv')  # Customer feature dataset\n",
    "\n",
    "# Drop unnecessary columns from the catalog and customer features datasets\n",
    "catalog.drop(columns=[\"categories\"], inplace=True)  # Remove the 'categories' column as it's not needed\n",
    "customer_features = customer_features.drop(columns=[\"frequent_product\", \"parent_category_id\"])  # Remove redundant columns\n",
    "\n",
    "# Load test data for filtering\n",
    "test_data = pd.read_csv('../data/test.csv')  # Test dataset containing customer-product pairs\n",
    "\n",
    "# Display the initial length of the merged data\n",
    "print(\"len merge:\", len(merged_data))\n",
    "\n",
    "# Select a subset of the merged data based on the defined size\n",
    "size10 = int(size * len(merged_data))  # Calculate the size for the subset\n",
    "merged_data = merged_data[:size10]  # Reduce the merged data to the subset\n",
    "\n",
    "# Filter the merged data to include only rows that exist in the test data\n",
    "selected_data = merged_data.merge(\n",
    "    test_data[[\"customer_id\", \"product_id\"]],  # Keep only customer_id and product_id from test data\n",
    "    on=[\"customer_id\", \"product_id\"],  # Match on customer_id and product_id\n",
    "    how=\"inner\"  # Perform an inner join to keep only matching rows\n",
    ")\n",
    "\n",
    "# Display the final length of the merged data\n",
    "print(\"len merge:\", len(merged_data))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:01.435902Z",
     "start_time": "2025-01-06T19:04:00.404997Z"
    }
   },
   "id": "97c0a03ab7a9af75",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge the selected data with the catalog to add product-related features\n",
    "selected_data = pd.merge(selected_data, catalog, on='product_id')  # Merge on 'product_id' to enrich the dataset with catalog information"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:01.443860Z",
     "start_time": "2025-01-06T19:04:01.436804Z"
    }
   },
   "id": "4fb860a2f6df0f18",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded successfully!\n",
      "Model loaded successfully!\n",
      "Encoders loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained scaler\n",
    "scaler = joblib.load(\"../scalers/scaler.pkl\")  # Load the MinMaxScaler used during training\n",
    "print(\"Scaler loaded successfully!\")\n",
    "\n",
    "# Redefine the function used in the Lambda layer\n",
    "def extract_column(column_index):\n",
    "    \"\"\"\n",
    "    Extract a specific column from the input tensor.\n",
    "    \n",
    "    Parameters:\n",
    "        column_index (int): Index of the column to extract.\n",
    "        \n",
    "    Returns:\n",
    "        Function: A lambda function to extract the specified column.\n",
    "    \"\"\"\n",
    "    return lambda x: x[:, column_index]\n",
    "\n",
    "# Specify the custom objects needed for loading the model\n",
    "custom_objects = {\"extract_column\": extract_column}\n",
    "\n",
    "# Load the trained model with the custom Lambda function\n",
    "model = load_model(\"../models/best_multiclass_model1.keras\", custom_objects=custom_objects, safe_mode=False)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load the encoders for customer and product IDs\n",
    "customer_encoder = joblib.load(\"../encoders/customer_encoder.pkl\")  # Encoder for customer IDs\n",
    "product_encoder = joblib.load(\"../encoders/product_encoder.pkl\")  # Encoder for product IDs\n",
    "\n",
    "print(\"Encoders loaded successfully!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:02.690846Z",
     "start_time": "2025-01-06T19:04:02.495594Z"
    }
   },
   "id": "86551e14769f93cb",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prediction_inputs(data, catalog, customer_catalog, n_steps):\n",
    "    \"\"\"\n",
    "    Prepares input data for predictions by extracting and encoding relevant features.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Input data containing customer and product IDs with weekly sales.\n",
    "        catalog (DataFrame): Product catalog with product-related features.\n",
    "        customer_catalog (DataFrame): Customer catalog with customer-related features.\n",
    "        n_steps (int): Number of historical weeks to include in the input.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Arrays for time series input, customer features, and product features.\n",
    "    \"\"\"\n",
    "    X_pred, customers_pred, products_pred = [], [], []\n",
    "\n",
    "    # Create dictionaries for customer and product features\n",
    "    customer_features_dict = customer_catalog.set_index('customer_id').to_dict(orient='index')\n",
    "    product_features_dict = catalog.set_index('product_id').to_dict(orient='index')\n",
    "\n",
    "    # Select columns representing weekly data\n",
    "    week_columns = [col for col in data.columns if '/' in col]\n",
    "\n",
    "    # Encode customer and product IDs\n",
    "    data[\"customer_id\"] = customer_encoder.transform(data[\"customer_id\"])  # Transform customer IDs\n",
    "    data[\"product_id\"] = product_encoder.transform(data[\"product_id\"])  # Transform product IDs\n",
    "\n",
    "    # Prepare inputs for prediction\n",
    "    for index in data.index:\n",
    "        # Extract the last n_steps weeks of data\n",
    "        weeks = data.loc[index, week_columns].values[-n_steps:]\n",
    "        customer_id = data.loc[index, \"customer_id\"]\n",
    "        product_id = data.loc[index, \"product_id\"]\n",
    "\n",
    "        # Get customer and product features\n",
    "        customer_features = customer_features_dict.get(customer_id, {})  # Default to empty dict if not found\n",
    "        product_features = product_features_dict.get(product_id, {})  # Default to empty dict if not found\n",
    "\n",
    "        # Only include rows with sufficient data\n",
    "        if len(weeks) == n_steps:\n",
    "            X_pred.append(weeks.reshape(-1, 1))  # Reshape weeks data for LSTM input\n",
    "            customers_pred.append([customer_id] + list(customer_features.values()))  # Include customer features\n",
    "            products_pred.append([product_id] + list(product_features.values()))  # Include product features\n",
    "\n",
    "    # Return prepared inputs as numpy arrays\n",
    "    return np.array(X_pred), np.array(customers_pred), np.array(products_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:03.304760Z",
     "start_time": "2025-01-06T19:04:03.300864Z"
    }
   },
   "id": "1d5066a5910e17e6",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def scale_and_reshape(scaler, X, num_steps, num_features):\n",
    "    \"\"\"\n",
    "    Scales and reshapes input data for model compatibility.\n",
    "\n",
    "    Parameters:\n",
    "        scaler (sklearn.preprocessing.MinMaxScaler): The scaler used to normalize the data.\n",
    "        X (numpy.ndarray): Input data to be scaled and reshaped.\n",
    "        num_steps (int): Number of time steps (sequence length) in the input data.\n",
    "        num_features (int): Number of features for each time step.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Scaled and reshaped data.\n",
    "    \"\"\"\n",
    "    # Reshape the input data into a 2D array for scaling\n",
    "    X_reshaped = X.reshape(-1, num_features)  # Flatten the sequences for scaling\n",
    "    \n",
    "    # Apply the scaler to normalize the data\n",
    "    X_scaled = scaler.transform(X_reshaped)  # Scale the data using the provided scaler\n",
    "    \n",
    "    # Reshape the scaled data back into the original sequence format\n",
    "    return X_scaled.reshape(-1, num_steps, num_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:04.149428Z",
     "start_time": "2025-01-06T19:04:04.145949Z"
    }
   },
   "id": "9db321172e94851c",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step\n",
      "Predicted Classes: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Prepare prediction data\n",
    "X_pred, customers_pred, products_pred = prepare_prediction_inputs(\n",
    "    selected_data, catalog, customer_features, n_steps\n",
    ")  # Extract time series, customer, and product features for prediction\n",
    "\n",
    "# Separate customer IDs and features\n",
    "customers_pred_ids = customers_pred[:, 0]  # Extract customer IDs\n",
    "customers_pred_features = customers_pred[:, 1:]  # Extract customer-related features\n",
    "\n",
    "# Scale and reshape input data for the model\n",
    "X_pred_scaled = scale_and_reshape(scaler, X_pred, n_steps, 1)  # Scale and reshape the time series data\n",
    "\n",
    "# Generate predictions using the trained model\n",
    "predictions = model.predict([customers_pred_ids, customers_pred_features, products_pred, X_pred_scaled])\n",
    "\n",
    "# Determine the predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# Display the predicted classes\n",
    "print(\"Predicted Classes:\", predicted_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:05.629977Z",
     "start_time": "2025-01-06T19:04:04.860629Z"
    }
   },
   "id": "2663caff36b36f4f",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of customers_pred: 978\n",
      "Length of products_pred: 978\n",
      "Length of predictions: 978\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of customers_pred: {len(customers_pred)}\")\n",
    "print(f\"Length of products_pred: {len(products_pred)}\")\n",
    "print(f\"Length of predictions: {len(predictions.argmax(axis=1))}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:06.363028Z",
     "start_time": "2025-01-06T19:04:06.359634Z"
    }
   },
   "id": "6c7b84a321542e0b",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[20664,   408,     4, ...,   284,     0,    66],\n       [28231,   193,     4, ...,   468,     3,   108],\n       [ 2690,   406,     4, ...,   491,     0,    66],\n       ...,\n       [23914,   408,     4, ...,   334,     0,    44],\n       [11178,   194,    10, ...,   503,     3,    85],\n       [32649,   498,     4, ...,   491,     3,    66]])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:07.116658Z",
     "start_time": "2025-01-06T19:04:07.113473Z"
    }
   },
   "id": "ec237a14b16a897b",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results saved to ../predictions/predictions.csv\n",
      "   id  customer_id  product_id  prediction\n",
      "0   0            0       20664           2\n",
      "1   1            0       28231           2\n",
      "2   2           13        2690           2\n",
      "3   3           15        1299           2\n",
      "4   4           15       20968           2\n",
      "Customers Prediction Shape: (978, 5)\n",
      "Products Prediction Shape: (978, 7)\n",
      "Scaled Prediction Shape: (978, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert prediction results to a DataFrame\n",
    "prediction_df = pd.DataFrame({\n",
    "    \"customer_id\": customer_encoder.inverse_transform(customers_pred_ids.astype(int)),  # Decode customer IDs back to their original values\n",
    "    \"product_id\": product_encoder.inverse_transform(products_pred[:, 0].astype(int)),  # Decode product IDs back to their original values\n",
    "    \"prediction\": predictions.argmax(axis=1)  # Get the predicted classes\n",
    "})\n",
    "\n",
    "# Merge the predictions with the test data\n",
    "test_data = test_data.drop(columns=['prediction'], errors='ignore')  # Remove the existing 'prediction' column if it exists\n",
    "test_data = test_data.merge(\n",
    "    prediction_df,\n",
    "    on=['customer_id', 'product_id'],  # Match predictions with test data by customer and product IDs\n",
    "    how='left'  # Use a left join to keep all rows in the test data\n",
    ")\n",
    "\n",
    "# Fill missing prediction values with 0 (for rows without predictions)\n",
    "test_data['prediction'] = test_data['prediction'].fillna(0).astype(int)  # Ensure predictions are integers\n",
    "\n",
    "# Save the prediction results to a CSV file\n",
    "output_path = \"../predictions/predictions.csv\"  # Define the output file path\n",
    "test_data.to_csv(output_path, index=False)  # Save the DataFrame to a CSV file\n",
    "print(f\"Prediction results saved to {output_path}\")\n",
    "\n",
    "# Print summary information\n",
    "print(test_data.head())  # Display the first few rows of the updated test data\n",
    "print(\"Customers Prediction Shape:\", customers_pred.shape)  # Expected: (number_of_samples, total_number_of_features)\n",
    "print(\"Products Prediction Shape:\", products_pred.shape)  # Expected: (number_of_samples, total_number_of_features)\n",
    "print(\"Scaled Prediction Shape:\", X_pred_scaled.shape)  # Expected: (number_of_samples, n_steps, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:07.976376Z",
     "start_time": "2025-01-06T19:04:07.956596Z"
    }
   },
   "id": "191ce989f6aabe23",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         20.         13.         10.          1.11111111]\n",
      " [ 0.         20.         13.         10.          1.11111111]\n",
      " [13.         67.         35.         12.          1.34      ]\n",
      " [15.         62.         38.         14.          1.26530612]\n",
      " [15.         62.         38.         14.          1.26530612]]\n",
      "[[20664   408     4     0   284     0    66]\n",
      " [28231   193     4     3   468     3   108]\n",
      " [ 2690   406     4     3   491     0    66]\n",
      " [ 1299  1056     4     0   474    -1   108]\n",
      " [20968  1315     4     0   444     0   144]]\n"
     ]
    }
   ],
   "source": [
    "print(customers_pred[:5]) \n",
    "print(products_pred[:5])  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:08.687846Z",
     "start_time": "2025-01-06T19:04:08.684256Z"
    }
   },
   "id": "fd1be4686f26770c",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    customer_id  product_id  prediction\n0             0       20664           2\n1             0       28231           2\n2            13        2690           2\n3            15        1299           2\n4            15       20968           2\n..          ...         ...         ...\n95          459       28347           2\n96          460       31525           2\n97          471        8615           2\n98          473       18630           2\n99          474       24405           2\n\n[100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>product_id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20664</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>28231</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>2690</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15</td>\n      <td>1299</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>20968</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>459</td>\n      <td>28347</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>460</td>\n      <td>31525</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>471</td>\n      <td>8615</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>473</td>\n      <td>18630</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>474</td>\n      <td>24405</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:09.470825Z",
     "start_time": "2025-01-06T19:04:09.462239Z"
    }
   },
   "id": "10241c687d451b1b",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    id  customer_id  product_id  prediction\n0    0            0       20664           2\n1    1            0       28231           2\n2    2           13        2690           2\n3    3           15        1299           2\n4    4           15       20968           2\n..  ..          ...         ...         ...\n95  95          459       28347           2\n96  96          460       31525           2\n97  97          471        8615           2\n98  98          473       18630           2\n99  99          474       24405           2\n\n[100 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>customer_id</th>\n      <th>product_id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>20664</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>28231</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>13</td>\n      <td>2690</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15</td>\n      <td>1299</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>15</td>\n      <td>20968</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>95</td>\n      <td>459</td>\n      <td>28347</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>96</td>\n      <td>460</td>\n      <td>31525</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>97</td>\n      <td>471</td>\n      <td>8615</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>98</td>\n      <td>473</td>\n      <td>18630</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>99</td>\n      <td>474</td>\n      <td>24405</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-06T19:04:10.111692Z",
     "start_time": "2025-01-06T19:04:10.105833Z"
    }
   },
   "id": "3b8f1ff202e523f9",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8c0d7f74871ff93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
